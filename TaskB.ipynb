{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHEsFRTolnRP"
      },
      "source": [
        "## Inizializzazione"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMnL_DkDlz81"
      },
      "source": [
        "Installiamo la libreria Transformers, libreria open source sviluppata da Hugging Face. Questa libreria è utile per l'implementazione di modelli di trasformatori pre-addestrati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mclz2zeIZPLE",
        "outputId": "d4e2768f-aaf2-45b8-de0c-846a6a664c35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvAZitGgmBFV"
      },
      "source": [
        "Configuro qui l'ambiente di lavoro su google Colab installando le principali librerie che andremo ad utilizzare.\n",
        "Carichiamo inoltre il dataset subtaskB_train.csv. Solo successivamente, andremo ad attuare la data augumentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajmwo1sdZT0O",
        "outputId": "9f922331-a384-4984-f680-e15b967b3872"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root = \"/content/gdrive/MyDrive/Colab Notebooks/torch/\"\n",
        "df = pd.read_csv(root+\"data/subtaskB_train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRGTC1pmmNuC"
      },
      "source": [
        "Instaziamo gli iperparametri. Molti degli iperparametri utilizzati sono stati\n",
        "scelti come vedremo successivamente, grazie all'utilizzo della grid search.\n",
        "Il language model utilizzato è invece xlm-roberta-base. Il motivo di questa scelta è legato alla struttura del dataset. Di fatti, se pur il dataset è un dataset in lingua italiana, contiene alcune parole o frasi in inglese il che ci ha portato ad una scelta di Roberta. Roberta è un modello multilingue, che riconosce anche le emoji, presenti nel nostro dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yxYNT_bYZW8D"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "  \"epochs\": 1000,\n",
        "  \"learning_rate\": 1e-3,\n",
        "  \"batch_size\": 64,\n",
        "  \"dropout\": 0.4,\n",
        "  \"stopwords\": True,\n",
        "  \"h_dim\": 512,\n",
        "  \"patience\": 300,\n",
        "  \"min_delta\": 0.01,\n",
        "  \"max_seq_length\": 512,\n",
        "  \"language_model\": \"xlm-roberta-base\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KHJDA_kp45q"
      },
      "source": [
        "# Data preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQpbuFmcm06G"
      },
      "source": [
        "La parte che vedremo fa parte del data pre processing. In questa cella in particolare andremo ad installare le principali librerie e a definire delle funzioni quali ad esempio \"extract_most_common_emoji\" che estrae le emoji più comuni , le conta e le ordina . Inoltre viene definito un pattern regex (regex_pattern) per identificare diversi elementi, tra cui le emoji.\n",
        "Viene creato un nuovo pattern regex (nuovo_regex_pattern) che aggiunge un'ulteriore parte per mantenere solo le emoji desiderate (quelle presenti in top_3_emoji)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ8DE0ZUn4S6"
      },
      "source": [
        "##Mantenimento Emoji più frequenti"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvde4YYaoJzD",
        "outputId": "8a178fb3-bb95-4812-f68f-626ac1d762db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nXtVh_J9oK6f"
      },
      "outputs": [],
      "source": [
        "last_df = df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y99uPKYJoMv7",
        "outputId": "c0c9fdf3-0b1e-4cef-fcec-6913d12790d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import emoji\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2z9A2FVp4m3",
        "outputId": "33638873-1597-4d39-f293-101cadfa0383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 3 Emojis: ['🇮🇹', '❗', '⚠️']\n",
            "Num Tot. Emoji:  694\n"
          ]
        }
      ],
      "source": [
        "def extract_most_common_emoji(text):\n",
        "  emojis = [element['emoji'] for element in emoji.emoji_list(text)]\n",
        "  return emojis\n",
        "\n",
        "# Funzione di sostituzione per mantenere solo le emoji desiderate\n",
        "def emoji_selezionata(match):\n",
        "  return match.group(0) if match.group(0) in top_3_emoji else ''\n",
        "\n",
        "# Esempio di utilizzo\n",
        "emojis_list = [extract_most_common_emoji(commento) for commento in df.comment_text]\n",
        "\n",
        "# Appiattimento della lista utilizzando itertools.chain\n",
        "emojis_list = list(chain(*emojis_list))\n",
        "\n",
        "# Creo dizionario contenente l'emoji con la propria frequenza\n",
        "emojis_dict = Counter(emojis_list)\n",
        "\n",
        "# Ordino il dizionario con le emoji\n",
        "emojis_dict = dict(sorted(emojis_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "# Stampa le prime 3 emoji più frequenti\n",
        "top_3_emoji = [emoji[0] for emoji in list(emojis_dict.items())[:3]]\n",
        "\n",
        "for id, commento, conspiracy in zip(df.index, df['comment_text'], df['conspiracy']):\n",
        "  # espressione regolare originale\n",
        "  regex_pattern = r'\\b\\d{2}/\\d{2}/\\d{4}\\b|\\S+@\\S+|\\b\\d{4}-\\d{2}-\\d{2}\\b|https?://\\S+|[^\\w\\s🇮🇹]'\n",
        "\n",
        "  # Aggiungi la parte per mantenere le emoji in top_3_emoji, incluso \"IT\"\n",
        "  nuovo_regex_pattern = f'{regex_pattern}|(?<=\\s)({\"|\".join(re.escape(emoji) for emoji in top_3_emoji)})(?=\\s)'\n",
        "\n",
        "  #print(\"\\n----------------------\\n Commento_vecchio: \", commento)\n",
        "\n",
        "  # unisco le 2\n",
        "  commento = re.sub(nuovo_regex_pattern, emoji_selezionata, commento)\n",
        "\n",
        "  df.loc[id, 'comment_text'] = commento\n",
        "\n",
        "  #print(\"\\n Commento_nuovo: \", df.loc[id, 'comment_text'])\n",
        "\n",
        "print(\"Top 3 Emojis:\", top_3_emoji)\n",
        "print(\"Num Tot. Emoji: \", len(emojis_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CigBOI-pnXKW"
      },
      "source": [
        "In questo blocco viene implementata la lemmatizzazione che è un processo linguistico che consiste nel ricondurre una parola al suo lemma ovvero alla sua forma base.\n",
        "Il codice importa il modello \"SpaCy\" in italiano per poi iterare su ogni \"commento\" del dataset e creare un oggetto \"doc\" che contiene un elenco di token cioè segmenti di testo che rappresentano le parole, le frasi o le parti del testo che verranno processati dal modello per poi essere uniti in un unica frase che andrà ad aggiornare il dataset con il commeto lemmatizzato"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BpNkfEjoYMR"
      },
      "source": [
        "##Lemmatizzazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymBF6SY7obLV",
        "outputId": "f914feb5-2286-4f15-cc48-d4a8757aa9f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-01-05 22:36:53.552351: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-05 22:36:53.552415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-05 22:36:53.554860: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-05 22:36:55.425874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting it-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.6.0/it_core_news_sm-3.6.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from it-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->it-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download it_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MiW4SWpzqHIY"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Carica il modello italiano di SpaCy\n",
        "nlp = spacy.load(\"it_core_news_sm\")\n",
        "\n",
        "for id, commento, conspiracy in zip(df.index, df['comment_text'], df['conspiracy']):\n",
        "  # Processa il testo con SpaCy\n",
        "  doc = nlp(commento)\n",
        "\n",
        "  #Unisci i vari token elaborati di una frase in un unica stringa\n",
        "  comment_lem = ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "  nuovo_record = {'ID': id, 'Comment text modificato': comment_lem, 'conspiracy': conspiracy}\n",
        "\n",
        "  df.loc[id, 'comment_text'] = comment_lem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X20zeR8iUBK7"
      },
      "source": [
        "##Conteggio parole piu frequenti"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLaMiKI6UG7H",
        "outputId": "6756bf2d-ad56-49ca-8f83-458d934947a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "covid_comments = df[df['conspiracy'] == 0]['comment_text']\n",
        "qanon_comments = df[df['conspiracy'] == 1]['comment_text']\n",
        "terrapiatta_comments=df[df['conspiracy']==2]['comment_text']\n",
        "prorussia_comments=df[df['conspiracy']==3]['comment_text']\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  tokens_list = nltk.word_tokenize(text, language='italian')\n",
        "  return ' '.join([word for word in tokens_list if not word.lower() in nltk.corpus.stopwords.words(\"italian\")])\n",
        "\n",
        "\n",
        "# Applico la rimozione delle stopwords ai commenti\n",
        "covid_comments = covid_comments.apply(remove_stopwords)\n",
        "qanon_comments = qanon_comments.apply(remove_stopwords)\n",
        "terrapiatta_comments = terrapiatta_comments.apply(remove_stopwords)\n",
        "prorussia_comments = prorussia_comments.apply(remove_stopwords)\n",
        "\n",
        "\n",
        "# Conto le parole più frequenti dopo la rimozione delle stopwords\n",
        "covid_comments_word_counts = Counter(\" \".join(covid_comments).split())\n",
        "top_covid_words = covid_comments_word_counts.most_common(10)\n",
        "\n",
        "qanon_word_counts = Counter(\" \".join(qanon_comments).split())\n",
        "top_qanon_words = qanon_word_counts.most_common(10)\n",
        "\n",
        "terrapiatta_word_counts = Counter(\" \".join(terrapiatta_comments).split())\n",
        "top_terrapiatta_words = terrapiatta_word_counts.most_common(10)\n",
        "\n",
        "prorussia_word_counts = Counter(\" \".join(prorussia_comments).split())\n",
        "top_prorussia_words = prorussia_word_counts.most_common(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAYalJCFU7TU"
      },
      "source": [
        "Le top parole, a parte alcuni molto evidenti come \"vaccino\" nel caso ad esempio del covid, dopo la lemmatizzazione diventano comuni per ogni label motivo per il quale abbiamo scelto di non inserirli nel nostro dataset finale di train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N6mofTTni1H"
      },
      "source": [
        "Funzione che viene utilizzata per rimuovere le stopword italiane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3GV_a99KZi94"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "  tokens_list = nltk.word_tokenize(text, language='italian')\n",
        "  return ' '.join([word for word in tokens_list if not word.lower() in nltk.corpus.stopwords.words(\"italian\")])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vg-WYksnm-l"
      },
      "source": [
        "Funzione che conta le parole che sono del tutto in maiuscolo. Si evince che le parole scritte del tutto in maiuscolo vengono scritte per enfatizzare un concetto con tono \"duro\", ad esempio in frasi cospirazioniste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OxRjX3ExZlet"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def count_uppercase_words(sentence):\n",
        "    words = re.findall(r'\\b[A-Z]+\\b', sentence)\n",
        "    return len(words)\n",
        "\n",
        "def count_words(sentence):\n",
        "    # Rimuove la punteggiatura dalla frase\n",
        "    sentence_without_punctuation = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "\n",
        "    #Rimuove date, link, email dalla frase\n",
        "    sentence_without_punctuation = re.sub(r'\\b\\d{2}/\\d{2}/\\d{4}\\b|\\S+@\\S+|\\b\\d{4}-\\d{2}-\\d{2}\\b|https?://\\S+', '', sentence)\n",
        "\n",
        "    # Divide la frase in parole\n",
        "    words = sentence_without_punctuation.split()\n",
        "\n",
        "    # Restituisce il numero totale di parole\n",
        "    return len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-94uDuNn75h"
      },
      "source": [
        "Funzione che genera gli embeddings. Dopo aver eseguito l'inferenza con il modello, la funzione estrae l'ultimo stato nascosto (last_hidden_states) dal tensore risultante"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbwuyZGk7Ii5"
      },
      "source": [
        "##Data agumentation sul dataset di train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oUKU6GC1Q1k"
      },
      "source": [
        "Definiamo in questa parte di codice le librerie necessarie e inizializziamo il modulo per l'unmaskin con il modello xlm-roberta-base. Questo sarà utile per la data augumentation da effettuare sul dataset di train per aumentare , nel nostro caso, raddoppiare il dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZR6Xe2LaqXXV"
      },
      "outputs": [],
      "source": [
        "(x_train, x_val, y_train, y_val) = train_test_split( df['comment_text'], df['conspiracy'], test_size=0.2, stratify=df['conspiracy'] , random_state=17)\n",
        "(x_train,x_test,y_train, y_test) = train_test_split( x_train, y_train, test_size=0.2, random_state=17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "c855a9eb49d547ca90cdca531fad7fa3",
            "be8fade3abad4fc182a607d6f5132cbf",
            "b177ba9e1b90480bb4e08bc5c9516f16",
            "51176ce9110d4633b4b9fca48734ffb4",
            "b3457a9ced00466482288e2762460920",
            "f64cb3e98aae459291b14fda3f4058c9",
            "fdb4889ede9f47e98b24006eeaa0cb5a",
            "1b1ea220db0a43f8b280c1cde92f3cbd",
            "24b89e1d148548eaa4011c1e2f3f6c32",
            "c57f7cf807b34082b2295ed1b9d4ebe8",
            "efa0ac4eca4e415ca7e618a519776a0d"
          ]
        },
        "id": "flLZM26K7HQB",
        "outputId": "8d80be1b-4407-4d26-cbcd-4d8c3575b5de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c855a9eb49d547ca90cdca531fad7fa3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Impostazione del percorso del nuovo file CSV\n",
        "percorso_file_dataAug = root + \"data/train_dataset_Aug.csv\"\n",
        "\n",
        "# Creazione di un nuovo DataFrame vuoto\n",
        "train_dataset_Aug = pd.DataFrame()\n",
        "\n",
        "# Inizializzazione del modulo per l'unmasking\n",
        "unmasker = pipeline('fill-mask', model='xlm-roberta-base')\n",
        "\n",
        "# Limita il numero di token da considerare\n",
        "limite_massimo = 512\n",
        "numero_minimo_token = 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rZo9RU9kkwZf"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text): # text = 'testo scritto tra virgolette'\n",
        "  tokens_list = nltk.word_tokenize(text, language='italian')\n",
        "  return ' '.join([word for word in tokens_list if not word.lower() in nltk.corpus.stopwords.words(\"italian\")])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPZuZQJMOfzx"
      },
      "source": [
        "Mantenimento emoji nel nuovo dataset di train, con data augumentation. Il lavoro è uguale a quello fatto precedentemente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "puY95_mnHLUd"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Create a reference variable for Class TweetTokenizer\n",
        "tk = TweetTokenizer()\n",
        "\n",
        "def gen_mask_sentence(sentence):\n",
        "\n",
        "  tokenized_result = tk.tokenize(sentence)\n",
        "\n",
        "  _tokenized_result = tokenized_result[:512]\n",
        "\n",
        "  #genero la maschera, il while mi permette di sostituire solo elementi che siano parole\n",
        "  rand_idx = random.randint(0, len(_tokenized_result) - 1)\n",
        "\n",
        "  flag = 0\n",
        "\n",
        "  while not _tokenized_result[rand_idx].isalpha() and flag < 100:\n",
        "    rand_idx = random.randint(0, len(_tokenized_result) - 1)\n",
        "    flag += 1\n",
        "\n",
        "  _tokenized_result[rand_idx] = '<mask>'\n",
        "\n",
        "  return ' '.join(_tokenized_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPfUcS8dk3dR",
        "outputId": "c3dfad46-1219-4a29-d7bc-30cf6ed6fd1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Int64Index([  0,   1,   2,   3,   4,   6,   8,  12,  13,  14,\n",
              "            ...\n",
              "            796, 798, 799, 800, 802, 803, 804, 805, 807, 808],\n",
              "           dtype='int64', length=518)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train = x_train.sort_index()\n",
        "y_train = y_train.sort_index()\n",
        "\n",
        "x_train.index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B0Q5LNy17aP"
      },
      "source": [
        "Viene eseguita quindi la fase di \"Data Augmentation\" sul set di addestramento x_train, splittato precedentemente e differenziato quindi dai dataset di validation/test.  In particolare, viene verificata la lunghezza minima dei token , se la lunghezza dei token è minore a 3, allora il commento non verrà mascherato , altrimenti, viene utilizzato l'unmasker con xlm-roberta-base e verrà mascherato un solo token. Il nuovo commento viene aggiunto in coda al commento originale e verrà memorizzato nel dataframe prima inizializzato \"train_dataset\" che verrà usato successivamente per la fase di train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcHB3wNfrmU9",
        "outputId": "fa5ff10e-8163-41e7-d2c6-cb4c721eb8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Errore: No mask_token (<mask>) found on the input\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "for commento, conspiracy in zip(x_train, y_train):\n",
        "\n",
        "  #rimuovo ritorni a capo\n",
        "  commento = commento.replace('\\n\\n', '').replace('\\n', '').replace('\\r', '')\n",
        "\n",
        "  # Aggiungo uno spazio dopo ogni apice, poichè si è notato che altrimenti viene tokenizzato male\n",
        "  commento = commento.replace(\"'\", \"' \").replace('\"', '\" ')\n",
        "\n",
        "  #Rimuovo le stopwords\n",
        "  commento = remove_stopwords(commento)\n",
        "\n",
        "  # Masking di una word nella frase\n",
        "  commento_with_mask = gen_mask_sentence(commento)\n",
        "\n",
        "  # Verifica la lunghezza minima dei token\n",
        "  if len(tk.tokenize(commento)) < numero_minimo_token:\n",
        "    record = {'comment_text': commento, 'conspiracy': conspiracy}\n",
        "    train_dataset_Aug = pd.concat([train_dataset_Aug, pd.DataFrame([record])], ignore_index=True)\n",
        "\n",
        "  else:\n",
        "    try:\n",
        "      new_commento = unmasker(commento_with_mask, tokenizer_kwargs={\"truncation\": True})\n",
        "\n",
        "      record = {'comment_text': commento, 'conspiracy': conspiracy}\n",
        "      train_dataset_Aug = pd.concat([train_dataset_Aug, pd.DataFrame([record])], ignore_index=True)\n",
        "\n",
        "      record = {'comment_text': new_commento[0]['sequence'], 'conspiracy': conspiracy}\n",
        "      train_dataset_Aug = pd.concat([train_dataset_Aug, pd.DataFrame([record])], ignore_index=True)\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "      print(f\"\\nErrore: {e}\")\n",
        "      record = {'comment_text': commento, 'conspiracy': conspiracy}\n",
        "      train_dataset_Aug = pd.concat([train_dataset_Aug, pd.DataFrame([record])], ignore_index=True)\n",
        "\n",
        "\n",
        "train_dataset_Aug.to_csv(percorso_file_dataAug, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "C7MxpBpZlFxN"
      },
      "outputs": [],
      "source": [
        "x_val = x_val.sort_index()\n",
        "y_val = y_val.sort_index()\n",
        "\n",
        "percorso_file_validation = root + \"data/validation_dataset.csv\"\n",
        "validation_dataset = pd.DataFrame()\n",
        "\n",
        "validation_dataset = pd.concat([validation_dataset, pd.DataFrame({'comment_text': x_val, 'conspiracy': y_val})], ignore_index=True)\n",
        "\n",
        "validation_dataset.to_csv(percorso_file_validation, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cAzx-P89lH0v"
      },
      "outputs": [],
      "source": [
        "x_test = x_test.sort_index()\n",
        "y_test = y_test.sort_index()\n",
        "\n",
        "percorso_file_test = root + \"data/test_dataset.csv\"\n",
        "test_dataset = pd.DataFrame()\n",
        "\n",
        "test_dataset = pd.concat([test_dataset, pd.DataFrame({'comment_text': x_test, 'conspiracy': y_test})], ignore_index=True)\n",
        "\n",
        "test_dataset.to_csv(percorso_file_test, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n09SJLdFqvoP"
      },
      "source": [
        "# Definizione classe Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0M6I3WqKrILe"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text): # text = 'testo scritto tra virgolette'\n",
        "  tokens_list = nltk.word_tokenize(text, language='italian')\n",
        "  return ' '.join([word for word in tokens_list if not word.lower() in nltk.corpus.stopwords.words(\"italian\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "igGT8ZdWZnXQ"
      },
      "outputs": [],
      "source": [
        "def gen_embeddings(input_id_text, attention_mask):\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = lm_model(input_id_text, attention_mask=attention_mask).last_hidden_state\n",
        "    last_hidden_states = last_hidden_states[:,0,:].squeeze(0)\n",
        "\n",
        "  return last_hidden_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ooq-XKfrQ4O"
      },
      "source": [
        "Classe My Dataset personalizzata. La classe accetta in input x che di base è la colonna \"comment_text\" e le labels \"conspiracy\". per ogni commento, nella colonna comment_text, vengono rimosse le stopword, ed eliminati i ritorno a capo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KiEAKWGOrRhp"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, x, y, stopwords):\n",
        "    #rimuovo ritorni a capo\n",
        "    x = [text.replace('\\n\\n', '').replace('\\n', '').replace('\\r', '') for text in tqdm(x, desc='Remove ritorno a capo ...')]\n",
        "    # rimuovo le stopwords se sono presenti\n",
        "    if stopwords:\n",
        "      text_clean = []\n",
        "      for sentence in tqdm(x, desc='Remove StopWord ... '):\n",
        "        sentence = remove_stopwords(sentence)\n",
        "        text_clean.append(sentence)\n",
        "    else:\n",
        "      text_clean = x\n",
        "\n",
        "    # genero gli embeddings\n",
        "    embeddings_list = []\n",
        "    for text in tqdm(text_clean, desc='Generation Embeddings ...'):\n",
        "      tokens = tokenizer([text], add_special_tokens=True, return_tensors='pt', padding='max_length', max_length = 512, truncation=True)\n",
        "      input_id_texts = tokens['input_ids'].squeeze(1).to(device)\n",
        "      mask_texts = tokens['attention_mask'].squeeze(1).to(device)\n",
        "      embeddings_list.append(gen_embeddings(input_id_texts, mask_texts))\n",
        "\n",
        "\n",
        "    self.embeddings = embeddings_list\n",
        "    self.labels = [torch.tensor(label) for label in y]\n",
        "\n",
        "  def classes(self):\n",
        "    return self.labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.embeddings[idx], np.array(self.labels[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF7yWailrfsY"
      },
      "source": [
        "##EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8yI6saHrih_"
      },
      "source": [
        "Funzione base con EarlyStopping. Verrà utilizzato per terminare l'addestramento quando il modello non migliora più"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "a7drT10hrjyg"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "  def __init__(self, patience=5, min_delta=0.0):\n",
        "\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.early_stop = False\n",
        "    self.min_validation_loss = torch.inf\n",
        "\n",
        "  def __call__(self, validation_loss):\n",
        "    if (validation_loss + self.min_delta) >= self.min_validation_loss:\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        self.early_stop = True\n",
        "        print(\"Early stop!\")\n",
        "    else:\n",
        "      self.min_validation_loss = validation_loss\n",
        "\n",
        "      self.counter = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfxHZqDEMRzw"
      },
      "source": [
        "##Definizione Classificatore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPAl4dASockI"
      },
      "source": [
        "Questo codice definisce una classe MyClassifier che estende nn.Module,un modello PyTorch. Il classificatore scelto è un classificatore di tipo feed-forward . Questa tipologia è largamente usata per l'analisi testuale e la categorizzazione dei testi per la sua flessibilità e per la capacità di apprendere rappresentazioni complesse nei dati. Tra i layer vengono utilizzati tecniche di regolarizzazione come dropout per prevenire l'overfitting , fondamentale in casi come questo in cui il dataset è di ridotte dimensioni e la batch normalization che viene utilizzata per migliorare la stabilità dell'addestramento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-tYM8b7zy-_0"
      },
      "outputs": [],
      "source": [
        "class MyClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, num_inputs, num_hidden, num_outputs,dropout):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
        "    self.linear2 = nn.Linear(num_hidden, num_hidden//2)\n",
        "    self.linear3 = nn.Linear(num_hidden//2, num_hidden//4)\n",
        "    self.linear4 = nn.Linear(num_hidden//4, num_outputs)\n",
        "    self.act_fn = nn.LeakyReLU(0.2)\n",
        "    self.batch_norm = nn.BatchNorm1d(num_hidden)\n",
        "    self.batch_norm2 = nn.BatchNorm1d(num_hidden//2)\n",
        "    self.batch_norm3 = nn.BatchNorm1d(num_hidden//4)\n",
        "    self.dropout  = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, input_texts):\n",
        "    x = input_texts\n",
        "    x = self.linear1(x)\n",
        "    x = self.batch_norm(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.act_fn(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.batch_norm2(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.act_fn(x)\n",
        "    x = self.linear3(x)\n",
        "    x = self.batch_norm3(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.act_fn(x)\n",
        "    x = self.linear4(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPTwt69krtmV"
      },
      "source": [
        "##Definizione delle varie funzioni per l'addestramento e il test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iyiSGSdxhtW"
      },
      "source": [
        "Definiamo la funzione f1_score_macro_sklearn che calcola la F1 Score macro utilizzando la libreria scikit-learn, come previsto dalla competizione EVALITA 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Z-HPB3AaqasT"
      },
      "outputs": [],
      "source": [
        "## F1 Score Macro\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def f1_score_macro_sklearn(y_true, y_pred):\n",
        "  y_pred = torch.round(y_pred.cpu()).numpy()\n",
        "  y_true = y_true.cpu().numpy()\n",
        "  return f1_score(y_true, y_pred, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU7U0-Irx1Gn"
      },
      "source": [
        "Definiamo ora un ciclo di addestramento personalizzato. Viene impostato il modello in modalità train e vengono inizializzati i valori di *train_loss,true_output,all_predictions e all_labels* utili per il calcolo delle performance. Il ciclo for attraversa il dataloader di addestramento che restituisce minibatch di dati ad ogni iterazione. Il modello viene eseguito sui dati di input(data_inputs, data_uppercase) per restituire l'output .\n",
        "Per il calcolo dell'accuracy viene utilizzata la softmax poichè il problema è un problema multiclasse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jMuPnhLcZzj5"
      },
      "outputs": [],
      "source": [
        "def my_train_loop(model, optimizer, train_dataloader, len_train_dataset, loss):\n",
        "  #imposto il modello in modalità allenamento\n",
        "  model.train()\n",
        "\n",
        "  train_loss = 0.0\n",
        "  true_output = 0.0\n",
        "  all_predictions = []\n",
        "  all_labels = []\n",
        "\n",
        "  for data_inputs, data_labels in train_dataloader:\n",
        "\n",
        "    # 1. Sposto i dati di input sul dispositivo\n",
        "    data_inputs = data_inputs.to(device)\n",
        "    data_labels = data_labels.to(device)\n",
        "\n",
        "    # 2. Eseguo il modello sui dati di input\n",
        "    output = model(data_inputs)\n",
        "\n",
        "    # 3. Calcolo la perdita\n",
        "    batch_loss = loss(output, data_labels) # il valore calcolato viene valutato per blocco\n",
        "\n",
        "    # 4. Eseguo la BackPropagation\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch_loss.backward()\n",
        "\n",
        "    # 5. Aggiorno i parametri (cioè i pesi)\n",
        "    optimizer.step()\n",
        "\n",
        "    # 6: Sommo il batch loss\n",
        "    train_loss += batch_loss.item()\n",
        "\n",
        "    # 7: Calcolo l'accuracy\n",
        "    softmax = nn.LogSoftmax(dim=1)\n",
        "    predictions= softmax(output).argmax(dim=1)\n",
        "    true_output += (predictions == data_labels).sum().item()\n",
        "\n",
        "    #8: Salvo predizioni e etichette per calcolare la F1-score macro\n",
        "    all_predictions.append(predictions)\n",
        "    all_labels.append(data_labels)\n",
        "\n",
        "  #Calcolo Loss e accuracy\n",
        "  train_loss = train_loss/len(train_dataloader)\n",
        "  train_acc = true_output/len_train_dataset\n",
        "\n",
        "  all_labels = torch.cat(all_labels, dim=0)\n",
        "  all_predictions = torch.cat(all_predictions, dim=0)\n",
        "  f1_macro = f1_score_macro_sklearn(all_labels, all_predictions)\n",
        "\n",
        "  return train_loss, train_acc, f1_macro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pGTkweSyshe"
      },
      "source": [
        "Questa pate del codice definisce invece una funzione, my_test_loop , utile per valutare le performance di un modello su un set di dati di test. Il modello viene impostato in modalità di valutazione con model.eval(), per cui non verranno attivate funzioni come batchNorm e dropout (a differenza di quando vengono attivate con model.train()). Vengono inizializzati i valori per il calcolo delle performance .\n",
        "La funzione restituisce l'accuratezza media, la perdita media e la F1-score macro ottenute durante la valutazione del modello su dati di test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "e9JyEUHUZ0SI"
      },
      "outputs": [],
      "source": [
        "def my_test_loop(model, dataset, len_dataset, loss):\n",
        "  #imposto il modello in modalità di valutazione\n",
        "  model.eval()\n",
        "\n",
        "  true_output, test_loss = 0., 0.\n",
        "  all_predictions = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data_inputs, data_labels in dataset:\n",
        "\n",
        "      # 1. Sposto i dati di input sul dispositivo\n",
        "      data_inputs = data_inputs.unsqueeze(0).to(device)\n",
        "      data_labels = torch.from_numpy(data_labels)\n",
        "      data_labels = data_labels.unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "      # 2. Eseguo il modello sui dati di input\n",
        "      output = model(data_inputs)\n",
        "      output = output\n",
        "\n",
        "      # 3. Calcolo la perdita\n",
        "      batch_loss = loss(output, data_labels)\n",
        "      test_loss += batch_loss.item()\n",
        "\n",
        "      #4: Calcolo l'accuracy\n",
        "\n",
        "      softmax = nn.LogSoftmax(dim=1)\n",
        "      predictions= softmax(output).argmax(dim=1)\n",
        "      true_output += (predictions == data_labels).sum().item()\n",
        "\n",
        "      #5: Salvo predizioni e etichette per calcolare la F1-score macro alla fine\n",
        "      all_predictions.append(predictions)\n",
        "      all_labels.append(data_labels)\n",
        "\n",
        "\n",
        "    test_loss = test_loss / len_dataset\n",
        "    test_acc = true_output / len_dataset\n",
        "\n",
        "    all_labels = torch.cat(all_labels, dim=0)\n",
        "    all_predictions = torch.cat(all_predictions, dim=0)\n",
        "    f1_macro = f1_score_macro_sklearn(all_labels, all_predictions)\n",
        "\n",
        "  return test_acc, test_loss, f1_macro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDhdL8SdzTe5"
      },
      "source": [
        "La funzione my_train_test esegue invece il processo di addestramento e valutazione di un modello su set di dati di addestramento, validazione e test.\n",
        "Vengono inizializzati i dataloader per il set di dati di addestramento (train_dataloader) e le liste per monitorare i valori di perdita, accuratezza e F1-score durante l'addestramento e la validazione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dpa3r6meZ2wy"
      },
      "outputs": [],
      "source": [
        "def my_train_test(model, optimizer, train_dataset, val_dataset, test_dataset, loss, num_epochs, early_stopping=None):\n",
        "\n",
        "  len_train_dataset = len(train_dataset)\n",
        "  len_val_dataset = len(val_dataset)\n",
        "  len_test_dataset = len(test_dataset)\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = hyperparameters['batch_size'], shuffle=True)\n",
        "\n",
        "  # liste dei valori di loss e accuracy epoca per epoca per il plot\n",
        "  list_train_loss, list_train_acc, list_val_loss, list_val_acc = [], [], [], []\n",
        "  list_train_f1, list_val_f1 = [], []\n",
        "\n",
        "  ## A. FASE DI ADDESTRAMENTO\n",
        "  for epoch in tqdm(range(num_epochs),  desc='training epch ...'):\n",
        "\n",
        "    # 1. ADDESTRAMENTO del Modello\n",
        "    train_loss, train_acc , train_f1= my_train_loop(model, optimizer, train_dataloader, len_train_dataset, loss)\n",
        "\n",
        "    # 2. Fase di VALIDAZIONE se è presente la callback di early stopping\n",
        "    if early_stopping != None:\n",
        "      val_acc, val_loss,val_f1 = my_test_loop(model, val_dataset, len_val_dataset, loss)\n",
        "      list_val_loss.append(val_loss)\n",
        "      list_val_acc.append(val_acc)\n",
        "      list_val_f1.append(val_f1)\n",
        "\n",
        "      # EARLY STOPPING\n",
        "      early_stopping(val_loss)\n",
        "      if early_stopping.early_stop:\n",
        "        break\n",
        "\n",
        "    # Aggiornamento delle lista che contengono le varie valutazioni del modello\n",
        "    list_train_loss.append(train_loss)\n",
        "    list_train_acc.append(train_acc)\n",
        "    list_train_f1.append(train_f1)\n",
        "\n",
        "  ## B. FASE DI TESTING\n",
        "  test_acc, test_loss, test_f1 = my_test_loop(model, test_dataset, len_test_dataset, loss)\n",
        "\n",
        "  return list_train_loss, list_train_acc, list_train_f1, list_val_loss,list_val_acc,list_val_f1, test_acc,test_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_0fYkd1L5xp"
      },
      "source": [
        "In questa parte del codice dopo aver importato le librerie necessarie, acquisiamo il device su cui effettueremo il training. Carichiamo le configurazioni (config), il tokenizzatore (tokenizer), e il modello di linguaggio (lm_model) utilizzando la libreria Transformers di Hugging Face.\n",
        "Definiamo inoltre il criterion, con la funzione di perdita CrossEntropyLoss e utilizziamo l'ottimizzatore Adam. Utilizziamo la CrossEntropyLoss perchè il problema è un problema multiclasse, viene utilizzata inoltre come funzione di attivazione del modello in combinazione con l'uso della softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0dh63cLLuZT"
      },
      "source": [
        "## Generazione dei dataset di train,validation e test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDgoVCI6r4Pe"
      },
      "source": [
        "##Avvio dell'addestramento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BvE9FttZ5da",
        "outputId": "ce012a13-b524-4cd0-fb45-adc899dd517e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyClassifier(\n",
            "  (linear1): Linear(in_features=768, out_features=512, bias=True)\n",
            "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (linear4): Linear(in_features=128, out_features=4, bias=True)\n",
            "  (act_fn): LeakyReLU(negative_slope=0.2)\n",
            "  (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            ")\n",
            "Numbero totale dei parametri: 560260\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "\n",
        "# Acquisiamo il device su cui effettueremo il training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "config = AutoConfig.from_pretrained(hyperparameters[\"language_model\"])\n",
        "tokenizer = AutoTokenizer.from_pretrained(hyperparameters[\"language_model\"])\n",
        "lm_model = AutoModel.from_pretrained(hyperparameters[\"language_model\"], config=config).to(device)\n",
        "modelClassifier = MyClassifier(num_inputs=768,num_hidden=hyperparameters[\"h_dim\"], num_outputs=4, dropout = hyperparameters[\"dropout\"]).to(device)\n",
        "\n",
        "\n",
        "print(modelClassifier)\n",
        "#numero totale dei parametri del modello\n",
        "total_params = sum(p.numel() for p in modelClassifier.parameters())\n",
        "print(f\"Numbero totale dei parametri: {total_params}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(modelClassifier.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "\n",
        "#callback di early stopping da passare al nostro metodo di addestramento\n",
        "early_stopping = EarlyStopping(patience=hyperparameters['patience'], min_delta=hyperparameters['min_delta'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sQlilQt0YdA"
      },
      "source": [
        "In questa parte del codice passiamo alla classe MyDataset i valori x_train,y_train,x_val,y_val,x_test,y_test precedentemente trovati e allo stesso modo, passiamo alla classe MyDataset i valori di comment_text e labels con data augumentation. Questo train_dataset diventa ora il dataset su cui effettuare il training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Y0WqmzHsL05E"
      },
      "outputs": [],
      "source": [
        "df_train_dataAug = pd.read_csv(root+ \"data/train_dataset_Aug.csv\")\n",
        "df_val = pd.read_csv(root+ \"data/validation_dataset.csv\")\n",
        "df_test = pd.read_csv(root+ \"data/test_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "WqZVg9DilZ0G"
      },
      "outputs": [],
      "source": [
        "x_train_aug, y_train_aug = df_train_dataAug['comment_text'], df_train_dataAug['conspiracy']\n",
        "x_val, y_val = df_val['comment_text'], df_val['conspiracy']\n",
        "x_test, y_test = df_test['comment_text'], df_test['conspiracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Le2pKoclbfL",
        "outputId": "5a8b9fb3-8e01-434b-94bb-c9ecfed42357"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Remove ritorno a capo ...: 100%|██████████| 1033/1033 [00:00<00:00, 330212.33it/s]\n",
            "Remove StopWord ... : 100%|██████████| 1033/1033 [00:15<00:00, 65.74it/s] \n",
            "Generation Embeddings ...: 100%|██████████| 1033/1033 [00:30<00:00, 33.91it/s]\n",
            "Remove ritorno a capo ...: 100%|██████████| 162/162 [00:00<00:00, 241463.13it/s]\n",
            "Remove StopWord ... : 100%|██████████| 162/162 [00:01<00:00, 107.73it/s]\n",
            "Generation Embeddings ...: 100%|██████████| 162/162 [00:04<00:00, 34.78it/s]\n",
            "Remove ritorno a capo ...: 100%|██████████| 130/130 [00:00<00:00, 235228.44it/s]\n",
            "Remove StopWord ... : 100%|██████████| 130/130 [00:01<00:00, 106.84it/s]\n",
            "Generation Embeddings ...: 100%|██████████| 130/130 [00:03<00:00, 34.79it/s]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = MyDataset(x_train_aug.values, y_train_aug.values, hyperparameters[\"stopwords\"])\n",
        "val_dataset = MyDataset(x_val.values, y_val.values, hyperparameters[\"stopwords\"])\n",
        "test_dataset = MyDataset(x_test.values, y_test.values, hyperparameters[\"stopwords\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HNs4mO2skcx"
      },
      "source": [
        "Avvio la fase di addestramento attraverso la funzione my_train_test e visualizzando i risultati attraverso i plot. Vengono stampati a schermo i risultati di accuracy e di f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZdgvqJ1slYn",
        "outputId": "8d69f9e9-ceff-46b6-e063-7e8fbb64a8d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training epch ...:  12%|█▏        | 119/1000 [00:21<02:26,  6.02it/s]"
          ]
        }
      ],
      "source": [
        "train_loss, train_acc,train_f1, validation_loss, validation_acc, val_f1, test_acc, test_f1 = my_train_test(modelClassifier,\n",
        "                                                                                            optimizer,\n",
        "                                                                                            train_dataset,\n",
        "                                                                                            val_dataset,\n",
        "                                                                                            test_dataset,\n",
        "                                                                                            criterion,\n",
        "                                                                                            hyperparameters[\"epochs\"],\n",
        "                                                                                            early_stopping)\n",
        "\n",
        "\n",
        "print(\"\\n---------------\\n\")\n",
        "print(f\"Test Accuracy: {test_acc}\\n\")\n",
        "print(f\"Test f1: {test_f1}\")\n",
        "print(\"\\n---------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sY7xY9bso15"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss, label='training loss')\n",
        "plt.plot(validation_loss, label='validation loss')\n",
        "#plt.plot(test_loss, label='test loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylim(0,1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43rcPai4sqiK"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_acc, label='training accuracy')\n",
        "plt.plot(validation_acc, label='validation accuracy')\n",
        "#plt.plot(test_acc, label='test accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0,1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ePIKc3oHX5f"
      },
      "source": [
        "##Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6quVMdYLJ0o"
      },
      "source": [
        "Grid Search con relativi plot. La Grid Search è stata implementata grazie alla libreria sklearn e ci ha permesso di individuare alcuni dei parametri migliori"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0aBmLSwaL6W"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definizione del set di iperparametri da testare\n",
        "param_grid = {\n",
        "    'learning_rate': [1e-3, 1e-5, 1e-6],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'dropout': [0.3, 0.4],\n",
        "    'h_dim': [512, 768],\n",
        "}\n",
        "\n",
        "# Creazione di tutte le combinazioni di iperparametri\n",
        "param_combinations = list(ParameterGrid(param_grid))\n",
        "\n",
        "# Lista per salvare i risultati della grid search\n",
        "results = []\n",
        "\n",
        "# Ciclo attraverso tutte le combinazioni di iperparametri\n",
        "for params in param_combinations:\n",
        "    print(\"\\nTesting with hyperparameters:\", params)\n",
        "\n",
        "    # Acquisiamo il device su cui effettueremo il training\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using {device} device\")\n",
        "\n",
        "    config = AutoConfig.from_pretrained(hyperparameters[\"language_model\"])\n",
        "    tokenizer = AutoTokenizer.from_pretrained(hyperparameters[\"language_model\"])\n",
        "    lm_model = AutoModel.from_pretrained(hyperparameters[\"language_model\"], config=config).to(device)\n",
        "    modelClassifier = MyClassifier(num_inputs=768, num_hidden=params[\"h_dim\"], num_outputs=4, dropout=params[\"dropout\"]).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(modelClassifier.parameters(), lr=params[\"learning_rate\"])\n",
        "\n",
        "    # Creiamo la callback di early stopping da passare al nostro metodo di addestramento\n",
        "    early_stopping = EarlyStopping(patience=hyperparameters['patience'], min_delta=hyperparameters['min_delta'])\n",
        "\n",
        "    # Addestramento del modello con i parametri correnti\n",
        "    train_loss, train_acc, train_f1, val_loss, val_acc,val_f1, test_acc, test_f1 = my_train_test(\n",
        "        modelClassifier, optimizer, train_dataset, val_dataset, test_dataset, criterion, 100, early_stopping\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"\\n---------------\\n\")\n",
        "    print(f\"Test Accuracy: {test_acc}\\n\")\n",
        "    print(f\"Test f1: {test_f1}\")\n",
        "    print(\"\\n---------------\\n\")\n",
        "\n",
        "    #Plot LOSS\n",
        "    plt.plot(train_loss, label='training loss')\n",
        "    plt.plot(val_loss, label='validation loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "    #Plot Acc\n",
        "    plt.plot(train_acc, label='training accuracy')\n",
        "    plt.plot(val_acc, label='validation accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "    # Salvataggio dei risultati nella lista\n",
        "    results.append({\n",
        "        'params': params,\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'val_loss': val_loss[-1],  # Prendi l'ultimo valore della lista di loss di validazione\n",
        "        'val_acc': val_acc[-1],    # Prendi l'ultimo valore della lista di accuracy di validazione\n",
        "        'test_f1': test_f1,\n",
        "        'test_acc': test_acc,\n",
        "    })\n",
        "\n",
        "# Trova la combinazione di iperparametri che ha ottenuto il miglior risultato sulla validazione\n",
        "best_result = max(results, key=lambda x: x['val_acc'])\n",
        "\n",
        "print(\"\\nBest hyperparameters:\", best_result['params'])\n",
        "print(\"Validation Accuracy:\", best_result['val_acc'])\n",
        "print(\"Test Accuracy:\", best_result['test_acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va7cNIRjLMr1"
      },
      "source": [
        "Carico dataset di test per avviare la submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA9Q3M_2F6xf"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(root+\"data/subtaskB_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5s2YPfGGDlN"
      },
      "source": [
        "#subtask B : fase di test per la challenge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "iwPNIauFtQdK"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(root+\"data/subtaskB_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h70bfcMwGG2H",
        "outputId": "804b9c87-23f1-4944-c41f-af14e0d78d9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 3 Emojis: ['⚡', '🇮🇹', '🤦\\u200d♂', '😳', '🔻', '🤣', '💪', '🧠', '📚', '😃', '😎', '😂', '😱', '▶️', '✔️', '🚨', '🔽', '🗂', '📃', '🔗', '😬', '🌺', '↪️', '💥', '‼️', '😁', '👏', '☝️', '😨', '🫁', '📊', '✝️', '❤️', '⁉️', '🥳', '👊🏾', '👤', '👉', '😉', '🫀', '🇮🇱', '⚠️', '👀', '🎯', '❗', '🙏🏻', '❓', '🔴', '😊', '📝', '📄', '🇬🇧', '😛', '🙏', '🗞', '🙄']\n",
            "Num Tot. Emoji:  56\n"
          ]
        }
      ],
      "source": [
        "emojis_list = [extract_most_common_emoji(commento) for commento in df_test.comment_text]\n",
        "\n",
        "\n",
        "emojis_list = list(chain(*emojis_list))\n",
        "emojis_list = list(set(emojis_list))\n",
        "\n",
        "top_3_emoji = emojis_list\n",
        "\n",
        "for id, commento in zip(df_test.index, df_test['comment_text']):\n",
        "  # espressione regolare originale\n",
        "  regex_pattern = r'\\b\\d{2}/\\d{2}/\\d{4}\\b|\\S+@\\S+|\\b\\d{4}-\\d{2}-\\d{2}\\b|https?://\\S+|[^\\w\\s🇮🇹]'\n",
        "\n",
        "  # Aggiungi la parte per mantenere le emoji in top_3_emoji, incluso \"IT\"\n",
        "  nuovo_regex_pattern = f'{regex_pattern}|(?<=\\s)({\"|\".join(re.escape(emoji) for emoji in top_3_emoji)})(?=\\s)'\n",
        "\n",
        "\n",
        "  # unisco le 2\n",
        "  commento = re.sub(nuovo_regex_pattern, emoji_selezionata, commento)\n",
        "  df_test.loc[id, 'comment_text'] = commento\n",
        "\n",
        "\n",
        "print(\"Top 3 Emojis:\", top_3_emoji)\n",
        "print(\"Num Tot. Emoji: \", len(emojis_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ak5gM71ZGLUn"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Carica il modello italiano di SpaCy\n",
        "nlp = spacy.load(\"it_core_news_sm\")\n",
        "\n",
        "for id, commento in zip(df_test.index, df_test['comment_text']):\n",
        "  # Processa il testo con SpaCy\n",
        "  doc = nlp(commento)\n",
        "\n",
        "  #Unisci i vari token elaborati di una frase in un unica stringa\n",
        "  comment_lem = ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "  df_test.loc[id, 'comment_text'] = comment_lem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV_ksZJV3m_b"
      },
      "source": [
        "Definiamo quindi una classe MyDataset_test diversa da quella precedentemente definita. In questa classe infatti, l'unico input sarà il comment_text e non verranno calcolate le performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "02y1nLjFGNRi"
      },
      "outputs": [],
      "source": [
        "class MyDataset_test(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, x, stopwords):\n",
        "    #rimuovo ritorni a capo\n",
        "    x = [text.replace('\\n\\n', '').replace('\\n', '').replace('\\r', '') for text in tqdm(x, desc='Remove ritorno a capo ...')]\n",
        "\n",
        "    # rimuovo le stopwords se sono presenti\n",
        "    if stopwords:\n",
        "      text_clean = []\n",
        "      for sentence in tqdm(x, desc='Remove StopWord ... '):\n",
        "        sentence = remove_stopwords(sentence)\n",
        "        text_clean.append(sentence)\n",
        "    else:\n",
        "      text_clean = x\n",
        "\n",
        "    # Conto il numero di uppercase e normalizzo il valore per la lunghezza della frase\n",
        "    num_uppercase = []\n",
        "    for sentence in tqdm(text_clean, desc=\"Count UpperCase ... \"):\n",
        "      count_words_ = count_words(sentence)\n",
        "      count_uppercase_words_ = count_uppercase_words(sentence)\n",
        "\n",
        "      if(count_words_ != 0 and count_words_ >= count_uppercase_words_):\n",
        "        num_uppercase.append(count_uppercase_words_ / count_words_)\n",
        "      else:\n",
        "        num_uppercase.append(0)\n",
        "\n",
        "    # genero gli embeddings\n",
        "    embeddings_list = []\n",
        "    for text in tqdm(text_clean, desc='Generation Embeddings ...'):\n",
        "      tokens = tokenizer([text], add_special_tokens=True, return_tensors='pt', padding='max_length', max_length = 512, truncation=True)\n",
        "      input_id_texts = tokens['input_ids'].squeeze(1).to(device)\n",
        "      mask_texts = tokens['attention_mask'].squeeze(1).to(device)\n",
        "      embeddings_list.append(gen_embeddings(input_id_texts, mask_texts))\n",
        "\n",
        "\n",
        "    self.embeddings = embeddings_list\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.embeddings[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "QVe0jx56t7FZ"
      },
      "outputs": [],
      "source": [
        "test = df_test['comment_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "L4YySQXvGVHD",
        "outputId": "a4320f0f-21d1-44c3-a6c7-f5b58703edc4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Remove ritorno a capo ...: 100%|██████████| 300/300 [00:00<00:00, 108604.45it/s]\n",
            "Remove StopWord ... : 100%|██████████| 300/300 [00:03<00:00, 93.96it/s]\n",
            "Count UpperCase ... : 100%|██████████| 300/300 [00:00<00:00, 9046.86it/s]\n",
            "Generation Embeddings ...:   0%|          | 0/300 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-400d9900ddfe>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stopwords\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-abd43fb0bfc8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, stopwords)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Generation Embeddings ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0minput_id_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0mmask_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0membeddings_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ],
      "source": [
        "dataset = MyDataset_test(test.values, hyperparameters[\"stopwords\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0djJ0Ck430W_"
      },
      "source": [
        "Attraverso la funzione my prediction_test definiamo come fatto precedentemente le performance del modello sul nuovo dataset di test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVbReefzGZzM"
      },
      "outputs": [],
      "source": [
        "def prediction_test(model, dataset):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  all_predictions = []\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data_inputs, data_uppercase in dataset:\n",
        "\n",
        "      # 1. Sposto i dati di input sul dispositivo\n",
        "      data_inputs = data_inputs.unsqueeze(0).to(device)\n",
        "\n",
        "      # 2. Eseguo il modello sui dati di input\n",
        "      output = model(data_inputs)\n",
        "      output = output.squeeze(0)\n",
        "\n",
        "\n",
        "      # 4. Calcolo accuracy\n",
        "      softmax = nn.LogSoftmax(dim=1)\n",
        "      predictions= softmax(output).argmax(dim=1)\n",
        "\n",
        "      # 5: Salva predizioni e etichette per calcolare la F1-score macro alla fine\n",
        "      all_predictions.append(predictions)\n",
        "\n",
        "\n",
        "  return all_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SEgFSe2GaeZ"
      },
      "outputs": [],
      "source": [
        "list_prediction_test = prediction_test(modelClassifier, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt5eA4reGcLS"
      },
      "outputs": [],
      "source": [
        "list_prediction_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x80O1KmGd6C"
      },
      "outputs": [],
      "source": [
        "lista_interi = [int(tensor.item()) for tensor in list_prediction_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUOxmL-PGga4"
      },
      "outputs": [],
      "source": [
        "lista_interi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22UJdtcOGhEW"
      },
      "outputs": [],
      "source": [
        "#Impostazione del percorso del nuovo file CSV\n",
        "percorso_file_output = root + \"data/output_testB.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYqQGLyN39kO"
      },
      "source": [
        "Salviamo infine i risultati ottenuti su un file csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tjsNAPMGkNP"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Salva la lista come file CSV\n",
        "with open(percorso_file_output, 'w', newline='') as file:\n",
        "  writer = csv.writer(file)\n",
        "  writer.writerow([\"id\", \"Expected\"])\n",
        "  writer.writerows([[i, valore] for i, valore in enumerate(lista_interi)])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WFYCxTkvopAy",
        "X20zeR8iUBK7",
        "xPTwt69krtmV",
        "J0dh63cLLuZT",
        "FDgoVCI6r4Pe",
        "_ePIKc3oHX5f",
        "H5s2YPfGGDlN"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b1ea220db0a43f8b280c1cde92f3cbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b89e1d148548eaa4011c1e2f3f6c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51176ce9110d4633b4b9fca48734ffb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c57f7cf807b34082b2295ed1b9d4ebe8",
            "placeholder": "​",
            "style": "IPY_MODEL_efa0ac4eca4e415ca7e618a519776a0d",
            "value": " 1.12G/1.12G [00:08&lt;00:00, 114MB/s]"
          }
        },
        "b177ba9e1b90480bb4e08bc5c9516f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b1ea220db0a43f8b280c1cde92f3cbd",
            "max": 1115567652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24b89e1d148548eaa4011c1e2f3f6c32",
            "value": 1115567652
          }
        },
        "b3457a9ced00466482288e2762460920": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be8fade3abad4fc182a607d6f5132cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f64cb3e98aae459291b14fda3f4058c9",
            "placeholder": "​",
            "style": "IPY_MODEL_fdb4889ede9f47e98b24006eeaa0cb5a",
            "value": "model.safetensors: 100%"
          }
        },
        "c57f7cf807b34082b2295ed1b9d4ebe8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c855a9eb49d547ca90cdca531fad7fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be8fade3abad4fc182a607d6f5132cbf",
              "IPY_MODEL_b177ba9e1b90480bb4e08bc5c9516f16",
              "IPY_MODEL_51176ce9110d4633b4b9fca48734ffb4"
            ],
            "layout": "IPY_MODEL_b3457a9ced00466482288e2762460920"
          }
        },
        "efa0ac4eca4e415ca7e618a519776a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f64cb3e98aae459291b14fda3f4058c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdb4889ede9f47e98b24006eeaa0cb5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
